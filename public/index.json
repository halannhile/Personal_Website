[{"authors":["admin"],"categories":null,"content":"I am an Environmental Specialist employed with the NJ Department of Environmental Protection. I work with the Division of Water Monitoring \u0026amp; Standards, where we research key water quality issues throughout the state. I utilize different computer software such as R, ArcGIS, Python, etc. to analyze the vast amount of data we collect.\nSolving key environmental issues and gaining actionable insight from data ignites a fire inside me. Being that a majority of my work involves making data driven decisions, I have gained extensive experience with data science techniques. I look forward everyday to contribute my skills and knowledge in an environment that will make a difference.\nWhen I am not crunching data or creating solutions for a better environment I can be found shredding the gnar at your local skatepark or ski resort. I love meeting new people and learning about new opportunities, so please feel free to reach out to me.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I am an Environmental Specialist employed with the NJ Department of Environmental Protection. I work with the Division of Water Monitoring \u0026amp; Standards, where we research key water quality issues throughout the state. I utilize different computer software such as R, ArcGIS, Python, etc. to analyze the vast amount of data we collect.\nSolving key environmental issues and gaining actionable insight from data ignites a fire inside me. Being that a majority of my work involves making data driven decisions, I have gained extensive experience with data science techniques.","tags":null,"title":"Kevin Zolea","type":"author"},{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536465600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536465600,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":["R","Roadsalt","Water Quality"],"content":"\rIntroduction\rBeing that I work for the Division of Water Monitoring \u0026amp; Standards, I have to analyze a lot of water quality data. A majority of the data is in house from the samples we collect but most times I have to download data from online sources. These online sources usually consist of the same organizations and most have a pretty straight forward way of getting access to the data. However, the process of going to that organization’s website and getting whatever data you need is a bit time consuming. I always look for ways that I can automate a task and not have to do the same things over and over again. It wasn’t until recently that I learned about the dataRetrieval package, which can do exactly that.\n\rWhat is the dataRetrieval package \u0026amp; how does it work?\rThe dataRetrieval package is a collection of functions to help retrieve U.S. Geological Survey (USGS) and U.S. Environmental Protection Agency (EPA) water quality and hydrology data from web services. With the dataRetrieval package you can discover, access, retrieve, and parse water data. The data comes from numerous different sources. The image below provides a nice overview of the different sources, data types, metadata, time series type, formats, and output.\nImage Credit: USGS\n\rI’m not going to go into all the different functions and usage of the dataRetrieval package but if you would like to learn more here are some of the sources I found the most useful:\n\rIntroduction to the dataRetrieval package\rdataRetrieval Tutorial\rUSGS PDF\r\r\rWhat I will show in this blog post\rIn this blog post I will discuss the usage of the readNWISuv() function and how to create a nice plot with the ggplot2 package. The readNWISuv() function imports data from the NWIS web service. Specifically, this function retrieves instantaneous water quality data. In order to use this function you must have the following arguments:\nreadNWISuv(siteNumbers, parameterCd, startDate = \u0026quot;\u0026quot;, endDate = \u0026quot;\u0026quot;,tz = \u0026quot;UTC\u0026quot;)\n\rsiteNumbers: A character vector of USGS site numbers (or multiple sites). This is usually an 8 digit number. You can use this map to find a site your interested in.\rparameterCd: Character USGS parameter code.This is usually an 5 digit number. To find a parameter code of interest you can type in parameterCdFile. This allows you to explore the USGS parameter codes.\rstartDate: character starting date for data retrieval in the form YYYY-MM-DD.\rendDate: character ending date for data retrieval in the form YYYY-MM-DD.\rtz: character to set timezone attribute of dateTime. Default is “UTC”, and converts the date times to UTC. There are numerous different possible values to use. For example, if you wanted it to be in Eastern Standard Time, you would use \u0026quot;America/New_York\u0026quot;\r\r\rInstall and load dataRetrieval package from cran\rinstall.packages(\u0026quot;dataRetrieval\u0026quot;) library(dataRetrieval)\n\rPull data with the readNWISuv() function\rFor my analysis I am going to pull continuous specific conductance (SC) data for a site of intereset in NJ. With this specific conductance data, I will calculate Total dissolved solids (TDS). I will do this by using an equation from a correlation I made between SC \u0026amp; TDS. Being that this isn’t the focus of the post, I will not go in detail about this. However, in a subsequent post I will explain how I did this.\nUSGS_continuous_sc_data\u0026lt;-readNWISuv(\u0026quot;01408029\u0026quot;,\u0026quot;00095\u0026quot;,tz = \u0026quot;America/New_York\u0026quot;)\nFor simplicity, I am only looking up one site and one type of parameter. You can look up multiple sites and parameters in one pull. Now lets take a look at a preview of the pull we just made.\n## agency_cd site_no dateTime X_00095_00000 X_00095_00000_cd\r## 1 USGS 01408029 2007-10-01 01:00:00 246 A\r## 2 USGS 01408029 2007-10-01 01:15:00 246 A\r## 3 USGS 01408029 2007-10-01 01:30:00 246 A\r## 4 USGS 01408029 2007-10-01 01:45:00 246 A\r## 5 USGS 01408029 2007-10-01 02:00:00 246 A\r## 6 USGS 01408029 2007-10-01 02:15:00 246 A\r## tz_cd\r## 1 America/New_York\r## 2 America/New_York\r## 3 America/New_York\r## 4 America/New_York\r## 5 America/New_York\r## 6 America/New_York\rThe names of the columns in the dataframe can be described as follows:\n\ragency_cd: The NWIS code for the agency reporting the data\n\rsite_no: The USGS site number\n\rdateTime: The date and time of the value converted to UTC\n\rX_00095_00000: The values of the parameter we gave to the function.\n\rX_00095_00000_cd: The statistic code\n\rtz_cd: The time zone code for dateTime\n\r\rYou can clean up the names with the reenameNWISColumns() function if you’d like.\n\rWe have the data… now what?\rNow that we have retrieved the data, we can now start manipulating it and create a plot. We will create the plot using the ggplot2 package. I mentioned before that I was going to calculate TDS. Just to give some background… the reason I am doing this is because of a project I am working on that deals with roadsalt. I figured I would include it in this post just to show that you have a great deal of options in R. I will discuss my roadsalt research in more detail in later posts!\n\rWhat is the ggplot2 package?\rThe ggplot2 package is a system for ‘declaratively’ creating graphics, based on “The Grammar of Graphics”. It is a great way to visualize the data you are analyzing. With ggplot2, you have a lot of flexibility with the amount of customization you can give your plot. In my opinion, I think it is very easy to learn and it produces beautiful high quality plots. To learn more about ggplot2, I recommend The Complete ggplot2 Tutorial.\n\rFull code used to create plot:\rlibrary(dataRetrieval)\rlibrary(ggplot2)\rlibrary(dplyr)\rlibrary(plyr)\r### Vector of sites with continuous specific conductance data ###\rsiteNumber\u0026lt;-c(\u0026quot;01408029\u0026quot;)\r### Parameter code for specific conductance ###\rparameterCd\u0026lt;-\u0026quot;00095\u0026quot;\r### Function that retrieves near real time continuous data for specific sites and parameters ###\rUSGS_continuous_sc_data\u0026lt;-readNWISuv(siteNumber,parameterCd,tz = \u0026quot;America/New_York\u0026quot;)\r### Filter dataframe ###\rUSGS_continuous_sc_data\u0026lt;-USGS_continuous_sc_data%\u0026gt;%\rdplyr::select(site_no,dateTime,X_00095_00000)%\u0026gt;%\rdplyr::rename(Site = site_no,Specific_conductance = X_00095_00000)\r### Calculate TDS based on continuous Specific Conductance data and eq from correlation plots ###\rfinal_USGS_data_TDS\u0026lt;-USGS_continuous_sc_data%\u0026gt;%\rdplyr::mutate(Calculated_TDS = Specific_conductance * 0.572 +6.19)\r### theme for plots ###\rgraph_theme_T\u0026lt;- theme_linedraw()+\rtheme(plot.title=element_text(size=15, face=\u0026quot;bold\u0026quot;,vjust=0.5,hjust = 0.5),\rplot.subtitle = element_text(size=15, face=\u0026quot;bold\u0026quot;,vjust=0.5,hjust = 0.5),\rpanel.grid.major.x = element_blank(),\rpanel.grid.minor.x = element_blank(),\rplot.background = element_blank(),\rpanel.background = element_blank(),\rplot.margin = unit(c(1.5,2,4,2), \u0026quot;lines\u0026quot;), legend.position = \u0026quot;bottom\u0026quot;,\rlegend.background = element_blank(),\rlegend.text=element_text(size=10, face=\u0026quot;bold\u0026quot;))\r### Make plot of data ###\rp\u0026lt;-ggplot(final_USGS_data_TDS, aes(x=dateTime,y=Calculated_TDS)) +\rgeom_line(aes(color = \u0026quot;USGS Continuous Data\u0026quot;),\rstat = \u0026quot;identity\u0026quot;,size=1.3)+\rscale_y_continuous(expand = c(0, 0), limits = c(0, max(final_USGS_data_TDS$Calculated_TDS)))+\rggtitle(\u0026quot;Total Dissolved Solids (TDS) Concentration (mg/L)\u0026quot;) +\rlabs(subtitle =paste(\u0026quot;USGS Site:\u0026quot;,final_USGS_data_TDS$Site,sep = \u0026#39;\u0026#39;))+\rxlab(\u0026quot;Year\u0026quot;) + ylab(\u0026quot;TDS Concentration (mg/L)\u0026quot;) +\rgeom_hline(aes(yintercept = 500,color=\u0026quot;Freshwater Aquatic Life Criteria for TDS = 500 mg/L\u0026quot;),size=1.3,alpha=0.6)+\rscale_color_manual(\u0026quot;\u0026quot;,\rvalues = c(\u0026quot;USGS Continuous Data\u0026quot;=\u0026quot;#037907\u0026quot;,\r\u0026quot;Freshwater Aquatic Life Criteria for TDS = 500 mg/L\u0026quot;=\u0026quot;red\u0026quot;))+\rgraph_theme_T p\r\r\rFinal Product:\r\rWhat is this plot showing?\rThis plot is showing the calculated TDS concentration for the selected site from 2007- present day. The red line indicates the Freshwater Aquatic Life Criteria for TDS. In the most simpliest terms, when the graph shows the TDS concentration (green line) going over the red line, TDS is over the standard.\n\rConclusion\rAs you can see, the dataRetrieval package is a very useful tool for water quality analysis. There is sooo much data you can obtain with just the ease of writing a few lines of code! I only touched base on 1 function! There are so many different functions you can use to gain access to all different types of water quality data. Definitly look over the resources I included in the beginning if you want to learn more. I know this post was very basic but I hope it has helped you in some way. If you have any questions feel free to reach out to me!\n\r","date":1550275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550275200,"objectID":"ed2449739ac1a8bf890cb19d2523cb35","permalink":"/post/usgs/using-the-usgs-dataretrieval-package-to-analyze-water-quality-trends/","publishdate":"2019-02-16T00:00:00Z","relpermalink":"/post/usgs/using-the-usgs-dataretrieval-package-to-analyze-water-quality-trends/","section":"post","summary":"Introduction on using the USGS dataRetrieval package to create a graph in ggplot2 to visualize total dissolved solid","tags":["R","Water Quality","Roadsalt"],"title":"Using the USGS dataRetrieval package to analyze continuous water quality data","type":"post"},{"authors":null,"categories":["blogdown","R","Hugo"],"content":"\rIntroduction:\rI have been told by a countless number of people that creating a personal website is a great way to showcase your skills and tell your story. I have been contemplating this for some time but kept procrastinating. The two biggest excuses I kept telling myself was that it would be too difficult \u0026amp; it would cost money. Boy was I wrong! It wasn’t until I started using R this past year that I realized how wrong I was. After a quick google search I came across the blogdown package. Blogdown is an amazing package in which you can create blogs and websites with R Markdown. Now I’m not saying that this stuff is extremely easy but if someone like myself(absolutely no website development knowledge) can do it, YOU CAN TOO! This blog post isn’t a sure fire way to do it but more of an overview of how I did it. I would also like to point out that this is a very basic introduction  to creating a blog/website with blogdown. The amount of things you can do with this package is almost endless.. especially if you have an understanding of CSS, HTML, and Javascript.\n\r\rPrerequisites:\rBefore you jump right in I recommend reading some online material and watching some youtube videos. Here is a list of the resources that helped me a lot.\n\rblogdown: Creating Websites with R Markdown\n\rUp and Running with Blogdown\n\rMaking a Website with Blogdown\n\rAcademic Theme Documentation (if your going to use the academic theme)\n\rMaking a Website Using Blogdown, Hugo, and GitHub pages\n\r\rOnce you review the above material you should have a pretty firm grasp on how to get the ball rolling. One of the biggest hurdles I had was creating my site with the Hugo Academic Theme. It wasn’t until I found this post on stackoverflow that I was able to figure out what the problem was. There was a breaking change in the hugo-academic theme, so I had to download the development version of blogdown. Not sure if this is still a thing but if you have the same problem definitly check out that post! The last piece of advice I will give before we get started is to make sure you check what the minimum hugo verision is for the theme you want to use. You can do this by going to the hugo website. Pick the theme you want to use and look at the line that says Minimum Hugo Verision: You can check what verision of hugo you have by using hugo_version() in R. Now lets get started!\n\rCreating a Repository and Cloning it\rI am going to assume you have used GitHub before, but if you haven’t, that is completely fine. You can check out this site to get a better understanding. Once you have a good understanding of GitHub and have an account created, you need to create a new repository. You can name this repository anything you want but it’s usually best practice to give it a meaningful name.\r\r\rOnce you click create repository you should be on your repository page.\nNow you have to create a local copy of your repository or in other words “clone it”. To do this click the green “Clone or Download” button on the right hand side and copy the url displayed.\r\r\rSo in order to “clone” the repo with the url that you just copied, you’re going to have to use git. If you don’t know anything about git, I recommend reading Happy Git and GitHub for the useR. Now you can do this in numerous different ways. You can open Terminal if you’re on a Mac, if you’re on windows you can open Git Bash or you can use the Terminal in Rstudio. Personally, I like to use Rstudio. So if you’re in Rstudio you have to make sure that you navigate to your working directory. You can see your working directory by typing pwd and to change directories use cd. Type git clone and then paste the URL that you copied before.\r\rThe command should look like this:\ngit clone https://github.com/zoleak/Personal_Website.git\nIf all went well then you should see a folder with the files in your repo in the directory that you chose.\n\r\rGetting started with the blogdown package in Rstudio\rThe time has finally come to start creating the site.\nOpen Rstudio and install blogdown. I recommend installing the development version, which can be done like this:\r\rremotes::install_github('rstudio/blogdown')\nSince blogdown is based on the static site generator Hugo, you need to install Hugo. You can easily do this by using a function in blogdown.\r\rinstall_hugo()\nUse the top menu buttons in Rstudio to browse to the directory on your computer where your GitHub repo is.\r\rFile-\u0026gt;New Project-\u0026gt;Existing Directory\nPick the theme you want to use. There are numerous different themes to pick from. I used the academic theme so I will use this one for the example. To browse themes click here\n\rCreate site using the new_site() function\n\r\rThere are a couple different options to create the site but I believe the best one is using the new_site() function. You can do this like so:\nblogdown::new_site(theme = \u0026quot;gcushen/hugo-academic\u0026quot;)\nAn example site should now be previewed in the Viewer panel of Rstudio.\nPersonalize the website\r\rI am not going to go over this because I am still learning how to do this myself. The main thing you should know is that you can edit the example site in any way you would like. You can change the title, fonts, color scheme, widgets used, etc. If you decide to use the Academic Theme look over the documentation . The author of the theme goes into great detail on how to get started and the different levels of customization you can do. The last thing I am going to touch base on is how to get your new site deployed to Netlify.\n\rDeploy in Netlify\rThere are a number of ways to deploy your new website but I personally like Netlify. Netlify allows you to connect to your GitHub repo, add custom build settings, and deploy your website. The best part about Netlify is that it’s free and extremely easy.\nWhen you are ready to deploy, commit your changes and push to GitHub, then go online to Netlify. You can commit your changes and push to GitHub all in Rstudio. Use the top menu button.\r\rTools-\u0026gt;Version Control-\u0026gt;Commit\nThis will bring up a new window:\nYou should see all your files there. Highlight all the files you want to commit and make sure they are set to staged. Add a commit message and then press commit. After you press commit a smaller window will pop up. Wait a couple seconds and let it do it’s thing. Once it’s done hit close. Lastly, click the push button. If you did this correctly the files will now be uploaded to your GitHub repo.\nGo to Netlify’s website and click on the sign up button and sign up using your existing GitHub account.\n\rLog in, and select: New site from Git-\u0026gt; Continuous Deployment: GitHub\n\rNetlify will then allow you to select from your existing GitHub repositories. Pick the repo you’ve been with.\n\r\r\rFinal Thoughts\rAs you can see, it isn’t as difficult as you may have thought to create your own website/blog. Blogdown is a great resource to utilize. Like I mentioned in the beginning, this is a very basic introduction into blogdown. There is so much more you can do to your website to make it awesome! I hope this post has helped you in some way in getting your website going. If you have an comments, constructive critism, questions, etc. please let me know and contact me.\n\r","date":1550016000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550016000,"objectID":"15f601f83218bd1c0681b98065fa1446","permalink":"/post/blogdown/creating-a-website-with-the-academic-theme-in-blogdown/","publishdate":"2019-02-13T00:00:00Z","relpermalink":"/post/blogdown/creating-a-website-with-the-academic-theme-in-blogdown/","section":"post","summary":"My thoughts and some guidance on how to set up a free website with blogdown, GitHub, and Netlify","tags":["blogdown","R"],"title":"Creating a website with the academic theme in blogdown","type":"post"},{"authors":null,"categories":null,"content":"Forecasting the diurnal cycle of precipitation over the continental United States (CONUS) is a problematic process for most global forecast systems. A majority tends to have a strong bias and they don’t provide a skilled prediction of the intensity, coverage and frequency of the diurnal cycle. Accurately forecasting the diurnal precipitation cycle, is closely related to the overall quality of the global forecast itself. Also, the accuracy of representation of physical processes in the models is indicative to the forecast skill. Major implementations have been made for the National Centers for Environmental Prediction (NCEP) operational Global Forecast System (GFS) throughout the years to make improvements to the diurnal cycle of precipitation. This study examines the diurnal cycle of precipitation over the CONUS during the winter and summer months of 2016-2017. The operational and experimental GFS will be analyzed and compared to the observed diurnal cycle of precipitation. To accomplish this, 3-hourly averaged accumulated precipitation vs. forecast hour plots, for the different models, were created. This allowed us to gain insight on how the skill of the models were performing, against the observations.\nThis study is expected to provide feedback to the model developers at NCEP’s Environmental Modeling Center (EMC) to inform (for making further) priorities for improvements to the GFS model, especially with the newly selected Next Generation Global Prediction System (NGGPS) Finite Volume Cube Sphere (FV3) modeling system. The NGGPS is a fully coupled system that will be designed to create useful forecast guidance out to 30 days, extend forecast skill beyond 8 to 10 days, and improve hurricane track/intensity forecast.\n","date":1549857600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549857600,"objectID":"c1fb692d6fa69e3357755f7490c034c5","permalink":"/project/ncep_project/","publishdate":"2019-02-11T00:00:00-04:00","relpermalink":"/project/ncep_project/","section":"project","summary":"My summer research project from my internship at the [National Centers for Environmental Prediction](https://www.ncep.noaa.gov)","tags":["Meteorology"],"title":"Analyzing the Diurnal Cycle of Precipitation in the NCEP Global Forecast System","type":"project"},{"authors":null,"categories":null,"content":"Easterly waves are weak troughs of low pressure that propagate westward across the Atlantic and East Pacific. These disturbances seed the majority of East Pacific and Atlantic tropical cyclones. Most easterly waves originate from Africa, although recent work indicates that they can also be locally generated in the East Pacific. This study analyzed the development and origins of the two precursor disturbances that eventually became Hurricanes Patricia and John in the East Pacific. Using TRMM precipitation and ERA-I reanalysis datasets, we investigate the local processes responsible for the organization of vorticity and precipitation in the initial disturbances from which these tropical cyclones formed. It is shown that although the easterly wave that formed Hurricane John originated from Africa, interactions with the Gulf of Papagayo jet influenced the intensification of the disturbance. The disturbance that formed Hurricane Patricia strengthened through complex interactions with the Gulf of Tehuantepec wind jet. Our results support the contention that both local and remote influences contribute to easterly wave formation in the East Pacific Ocean.\n","date":1549857600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549857600,"objectID":"8d957c55f0b1df929e4349f005683012","permalink":"/project/colorado_state_project/","publishdate":"2019-02-11T00:00:00-04:00","relpermalink":"/project/colorado_state_project/","section":"project","summary":"My summer undergraduate research internship at Colorado State University in the [Department of Atmospheric Science](https://www.atmos.colostate.edu). Sponsored by the National Science Foundation.","tags":["Meteorology"],"title":"Case Study Analysis of Easterly Wave Formation in the East Pacific","type":"project"},{"authors":null,"categories":null,"content":"","date":1549857600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549857600,"objectID":"e2af40999e0b34807d2c85cd5d1a55bb","permalink":"/project/census_app/","publishdate":"2019-02-11T00:00:00-04:00","relpermalink":"/project/census_app/","section":"project","summary":"A simple shiny app that uses the tidycensus package to analyze some census data from the [US Census Bureau API](https://www.census.gov/data.html)","tags":["Shiny"],"title":"US Census Bureau Shiny App","type":"project"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536465600,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00-04:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":[],"categories":null,"content":"\rClick on the Slides button above to view the built-in slides feature.\n\r\rSlides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483246800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483246800,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00-05:00","relpermalink":"/talk/example/","section":"talk","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["GA Cushen"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1441080000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441080000,"objectID":"d77fa4a74076ffcd7ca6c21cfc27a4b2","permalink":"/publication/person-re-id/","publishdate":"2015-09-01T00:00:00-04:00","relpermalink":"/publication/person-re-id/","section":"publication","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"A Person Re-Identification System For Mobile Devices","type":"publication"},{"authors":["GA Cushen","MS Nixon"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1372651200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372651200,"objectID":"2b4d919e3cf73dfcd0063c88fe01cb00","permalink":"/publication/clothing-search/","publishdate":"2013-07-01T00:00:00-04:00","relpermalink":"/publication/clothing-search/","section":"publication","summary":"We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.","tags":[],"title":"Mobile visual clothing search","type":"publication"}]